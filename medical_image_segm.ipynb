{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06b0833e-116b-421a-9c7a-e4c162fdaa84",
   "metadata": {},
   "source": [
    "<h1 style = \"text-align: center;\">Endoscope Semantic Segmentation</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4377cbb9-eac8-4d33-b29c-1efab39580b0",
   "metadata": {},
   "source": [
    "\n",
    "  <h2>Project Scope and Overview</h2>\n",
    "  <p>This project focuses on advancing semantic segmentation in medical imaging, particularly for computer-assisted surgery. The main objective is to develop neural network models that can accurately segment surgical images into distinct classes, such as various tissues, surgical instruments, blood vessels, and other critical anatomical structures. By improving segmentation accuracy, the project aims to enhance real-time surgical navigation and safety, providing essential support for clinical decision-making during operations.</p>\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6747cd6-5773-4842-b39e-782f9b0bd252",
   "metadata": {},
   "source": [
    "<body>\n",
    "\n",
    "  <h2>Dataset Overview</h2>\n",
    "<p>\n",
    "  The CholecSeg8K dataset is organized into a clear hierarchical structure, making it easy to locate and use the data. Below is a breakdown of its organization:\n",
    "</p>\n",
    "<ul>\n",
    "  <li>\n",
    "    <strong>Top-Level Directories:</strong>\n",
    "    <ul>\n",
    "      <li>Folders are labeled as <em>video01</em>, <em>video02</em>, etc., where each folder represents a complete surgical video clip.</li>\n",
    "    </ul>\n",
    "  </li>\n",
    "  <li>\n",
    "    <strong>Segment Directories:</strong>\n",
    "    <ul>\n",
    "      <li>Within each video folder, the video is divided into several segments.</li>\n",
    "      <li>Each segment directory is named with the video ID and the starting frame number (for example, <em>video01_00080</em> indicates that the segment starts at frame 80).</li>\n",
    "    </ul>\n",
    "  </li>\n",
    "  <li>\n",
    "    <strong>Frame and Image Files:</strong>\n",
    "    <ul>\n",
    "      <li>Each segment directory contains <strong>80 consecutive frames</strong> extracted from the video.</li>\n",
    "      <li>For every frame, there are <strong>4 image files</strong>:\n",
    "        <ul>\n",
    "          <li>The raw image frame</li>\n",
    "          <li>The annotation tool mask (the original hand-drawn annotation)</li>\n",
    "          <li>The color mask (used for visualization, where classes are painted in distinct colors)</li>\n",
    "          <li>The watershed mask (used for processing, where each pixel value corresponds to a class ID)</li>\n",
    "        </ul>\n",
    "      </li>\n",
    "      <li>This results in <strong>80 frames × 4 images per frame = 320 images</strong> in each segment directory.</li>\n",
    "    </ul>\n",
    "  </li>\n",
    "  <li>\n",
    "    <strong>Annotations:</strong>\n",
    "    <ul>\n",
    "      <li>Each frame is annotated at the pixel level for 13 distinct classes (e.g., tissue, instruments, blood vessels, etc.).</li>\n",
    "      <li>The annotations are presented in both the color and watershed masks, ensuring clear class identification for both visualization and automated processing.</li>\n",
    "    </ul>\n",
    "  </li>\n",
    "</ul>\n",
    "<p>\n",
    "  This structured, high-quality organization facilitates the development and training of advanced neural networks for precise semantic segmentation in surgical environments.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bfb569-7174-417f-a7a9-eb823aa4c770",
   "metadata": {},
   "source": [
    "<body>\n",
    "  <div class=\"gallery\">\n",
    "    <img src=\"./Images/Fig1.png\" alt=\"Figure 1\">\n",
    "    <img src=\"./Images/Fig2.png\" alt=\"Figure 2\">\n",
    "    <img src=\"./Images/Fig3.png\" alt=\"Figure 3\">\n",
    "  </div>\n",
    "</body>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b97e5b8-58f5-4829-a28c-83df4af1dc00",
   "metadata": {},
   "source": [
    "\n",
    "  <h2>Class Information Table</h2>\n",
    "  <p>Table I shows the corresponding class names of the class numbers in Figure 1, 2, 3 and the RGB hex code in the watershed masks:</p>\n",
    "  <table>\n",
    "    <thead>\n",
    "      <tr>\n",
    "        <th>Class Number</th>\n",
    "        <th>Class Name</th>\n",
    "        <th>RGB Hexcode</th>\n",
    "      </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "      <tr>\n",
    "        <td>Class 0</td>\n",
    "        <td>Black Background</td>\n",
    "        <td>#505050</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td>Class 1</td>\n",
    "        <td>Abdominal Wall</td>\n",
    "        <td>#111111</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td>Class 2</td>\n",
    "        <td>Liver</td>\n",
    "        <td>#212121</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td>Class 3</td>\n",
    "        <td>Gastrointestinal Tract</td>\n",
    "        <td>#131313</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td>Class 4</td>\n",
    "        <td>Fat</td>\n",
    "        <td>#121212</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td>Class 5</td>\n",
    "        <td>Grasper</td>\n",
    "        <td>#313131</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td>Class 6</td>\n",
    "        <td>Connective Tissue</td>\n",
    "        <td>#232323</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td>Class 7</td>\n",
    "        <td>Blood</td>\n",
    "        <td>#242424</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td>Class 8</td>\n",
    "        <td>Cystic Duct</td>\n",
    "        <td>#252525</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td>Class 9</td>\n",
    "        <td>L-hook Electrocautery</td>\n",
    "        <td>#323232</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td>Class 10</td>\n",
    "        <td>Gallbladder</td>\n",
    "        <td>#222222</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td>Class 11</td>\n",
    "        <td>Hepatic Vein</td>\n",
    "        <td>#333333</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td>Class 12</td>\n",
    "        <td>Liver Ligament</td>\n",
    "        <td>#050505</td>\n",
    "      </tr>\n",
    "    </tbody>\n",
    "  </table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f39d5f-5a33-4e0c-869d-e29b3e6ca0b3",
   "metadata": {},
   "source": [
    "<h2>Mask Overview</h2>\n",
    "<p>\n",
    "  The table below summarizes the three types of masks that accompany each image frame, along with their descriptions and corresponding images.\n",
    "</p>\n",
    "<table border=\"1\" cellspacing=\"0\" cellpadding=\"10\">\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>Mask Name</th>\n",
    "      <th>Description</th>\n",
    "      <th>Image</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>Original Image Frame</td>\n",
    "      <td>This is the raw endoscopic image captured during the surgery.</td>\n",
    "      <td><img src=\"./Images/frame_100_endo.png\" alt=\"Original Endoscopic Image\" width=\"200\"></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>1. Annotation Tool Mask</td>\n",
    "      <td>\n",
    "        <ul>\n",
    "          <li>This is the original hand-drawn mask created during the annotation process.</li>\n",
    "          <li>It contains detailed pixel-level annotations drawn by experts.</li>\n",
    "          <li>It serves as the basis for generating the other two masks.</li>\n",
    "        </ul>\n",
    "      </td>\n",
    "      <td><img src=\"./Images/frame_100_endo_mask.png\" alt=\"Annotation Tool Mask\" width=\"200\"></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>2. Color Mask</td>\n",
    "      <td>\n",
    "        <ul>\n",
    "          <li>Derived from the annotation tool mask.</li>\n",
    "          <li>It assigns a unique color to each class (e.g., tissue, instrument, blood vessel) based on predefined IDs.</li>\n",
    "          <li>This facilitates visual inspection and interpretation of the segmentation results.</li>\n",
    "        </ul>\n",
    "      </td>\n",
    "      <td><img src=\"./Images/frame_100_endo_color_mask.png\" alt=\"Color Mask\" width=\"200\"></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>3. Watershed Mask</td>\n",
    "      <td>\n",
    "        <ul>\n",
    "          <li>Also generated from the annotation tool mask.</li>\n",
    "          <li>It assigns a uniform pixel value (the same across all three RGB channels) to each class.</li>\n",
    "          <li>These numerical values represent the class IDs, making it ideal for automated processing and further analysis.</li>\n",
    "        </ul>\n",
    "      </td>\n",
    "      <td><img src=\"./Images/frame_100_endo_watershed_mask.png\" alt=\"Watershed Mask\" width=\"200\"></td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc5fb56e-8694-4e7e-aa2f-47cad3166e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "362f6258-1257-4604-b45e-6e672dae64e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pandas.plotting import scatter_matrix\n",
    "from seaborn import scatterplot, heatmap\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from joblib import dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d704b8d7-cd45-49f9-94e6-d8ecd9f3224b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "  base_dir = \"./drive/My Drive/Colab Notebooks/\" \n",
    "else:\n",
    "  base_dir = \".\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26241dd-0b1a-4df1-a402-f141f94d0988",
   "metadata": {},
   "source": [
    "<h2>I. Data Engineering</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211e90db-9e40-4ecb-845f-6dc8c33c5699",
   "metadata": {},
   "source": [
    "<h3>a. Data Cleaning</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64479c88-724b-4532-b2f9-46c1433fdff1",
   "metadata": {},
   "source": [
    "<h3>b. Dataset Exploration</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5abccf5-b68e-4f2d-b292-d73f9e2606f5",
   "metadata": {},
   "source": [
    "<h3>d. Impute/Replace Values</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba90025-73a8-4530-94c9-5b63dcaad603",
   "metadata": {},
   "source": [
    "<h3>e. Feature Engineering</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf939c1-c7ed-4125-b908-4d06424d2051",
   "metadata": {},
   "source": [
    "<h3>f. Data preprocessing</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cefe86-9050-4850-99d3-7496bdd1132e",
   "metadata": {},
   "source": [
    "<h2>II. Modeling</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22109ff3-9aa4-4086-a41a-c72ab730ef5f",
   "metadata": {},
   "source": [
    "<h4>Choosing a Backbone</h4>\n",
    "<p>\n",
    "  <strong>U-Net:</strong> A popular choice for medical image segmentation due to its encoder–decoder architecture with skip connections that preserve spatial context.\n",
    "</p>\n",
    "<p>\n",
    "  <strong>Alternative Models:</strong> Consider architectures such as DeepLabV3, FCN, or attention-based segmentation networks if you need advanced performance or have specific requirements.\n",
    "</p>\n",
    "<h4>Customization</h4>\n",
    "<ul>\n",
    "  <li>Adapt the network’s output layer to match the number of segmentation classes.</li>\n",
    "  <li>Configure the input layer to accept 3 channels *H*W (RGB) for the image data.</li>\n",
    "  <li>Set the output layer to produce 13 channels (one for each class).</li>\n",
    "  <li>Experiment with deeper or shallower networks depending on available GPU memory and desired resolution.</li>\n",
    "</ul>\n",
    "<h4>U-Net Specific Customizations</h4>\n",
    "<ul>\n",
    "  <li>\n",
    "    <strong>Activation Functions:</strong>\n",
    "    <ul>\n",
    "      <li>Use ReLU (or variants like Leaky ReLU or ELU) in the encoder and decoder layers.</li>\n",
    "      <li>Apply a softmax activation function in the final layer to obtain class probabilities for multi-class segmentation.</li>\n",
    "    </ul>\n",
    "  </li>\n",
    "  <li>\n",
    "    <strong>Loss Functions:</strong>\n",
    "    <ul>\n",
    "      <li>Consider using categorical cross-entropy, Dice loss, or a combination to better handle class imbalance.</li>\n",
    "    </ul>\n",
    "  </li>\n",
    "  <li>\n",
    "    <strong>Regularization:</strong>\n",
    "    <ul>\n",
    "      <li>Incorporate dropout layers to reduce overfitting.</li>\n",
    "      <li>Add batch normalization to stabilize and accelerate training.</li>\n",
    "    </ul>\n",
    "  </li>\n",
    "  <li>\n",
    "    <strong>Optimization:</strong>\n",
    "    <ul>\n",
    "      <li>Utilize optimizers such as Adam or SGD with momentum.</li>\n",
    "      <li>Implement learning rate scheduling to adjust the learning rate during training for improved convergence.</li>\n",
    "    </ul>\n",
    "  </li>\n",
    "  <li>\n",
    "    <strong>Network Architecture:</strong>\n",
    "    <ul>\n",
    "      <li>Adjust the number of layers and filters per layer to balance between model complexity and computational efficiency.</li>\n",
    "      <li>Consider using residual connections or attention mechanisms if additional performance improvements are needed.</li>\n",
    "    </ul>\n",
    "  </li>\n",
    "  <li>\n",
    "    <strong>Data Augmentation:</strong>\n",
    "    <ul>\n",
    "      <li>Apply techniques like rotation, flipping, and scaling to increase the diversity of the training data and improve model robustness.</li>\n",
    "    </ul>\n",
    "  </li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0c5053-c58e-4dec-94b7-4888b94a836c",
   "metadata": {},
   "source": [
    "<h4 style=\"color:red; font-style:italic;\">Method 1: <u>  </u></43>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a3fc05-e843-478f-a882-b392c68d8d31",
   "metadata": {},
   "source": [
    "<h5>a. Error Estimation & Model Selection</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb250eb0-5ac1-4c04-9c2f-ee50401a76e2",
   "metadata": {},
   "source": [
    "<h5>b. Training</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31737028-5b84-4ae6-81d1-0e0dec70111d",
   "metadata": {},
   "source": [
    "<h5>c. Evaluation</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757a370f-7129-42cd-ad9e-5b828aec6cd6",
   "metadata": {},
   "source": [
    "<h4 style=\"color:red; font-style:italic;\">Method 2: <u></u></h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443e87b6-3dfe-4aae-9cf7-138fd524378b",
   "metadata": {},
   "source": [
    "<h5>a. Error Estimation & Model Selection</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7247551-d36e-4d2c-9a6e-51dd4b424dc8",
   "metadata": {},
   "source": [
    "<h5>b. Training</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1227768-93db-4869-8d58-d809ddb6693d",
   "metadata": {},
   "source": [
    "<h5>c. Evaluation</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7265255e-6ae3-48d4-b8ee-6a84ce726a05",
   "metadata": {},
   "source": [
    "<h4 style=\"color:red; font-style:italic;\">Method 3: <u><u></h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c0837f-e6b3-48c2-b089-44b06924db37",
   "metadata": {},
   "source": [
    "<h4>Confidence Intervals </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bebdbe5-cd83-4fc6-8f1e-aaaff10bd7d6",
   "metadata": {},
   "source": [
    "<p style=\"color:blue; font-style:italic;\">Source: https://github.com/rasbt/machine-learning-notes/blob/main/evaluation/ci-for-ml/confidence-intervals-for-ml.ipynb</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0afc58-c2fc-4371-a665-93e20bfc824e",
   "metadata": {},
   "source": [
    "<h2>III. Deploy</h2>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
