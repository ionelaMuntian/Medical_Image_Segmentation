{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4377cbb9-eac8-4d33-b29c-1efab39580b0",
   "metadata": {},
   "source": [
    "\n",
    "  <h2>\n",
    "      \n",
    "  </h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b0833e-116b-421a-9c7a-e4c162fdaa84",
   "metadata": {},
   "source": [
    "<h1 style = \"text-align: center;\">Endoscope Semantic Segmentation</h1>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f121fa4-df05-437f-88bc-187b1ee2befd",
   "metadata": {},
   "source": [
    "\n",
    "  <h2>\n",
    "      \n",
    "  </h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d26e0ba-17c1-4d21-84b4-83f5ed34df9e",
   "metadata": {},
   "source": [
    "\n",
    "  <h2>Project Scope and Overview</h2>\n",
    "  <p>This project focuses on advancing semantic segmentation in medical imaging, particularly for computer-assisted surgery. The main objective is to develop neural network models that can accurately segment surgical images into distinct classes, such as various tissues, surgical instruments, blood vessels, and other critical anatomical structures. By improving segmentation accuracy, the project aims to enhance real-time surgical navigation and safety, providing essential support for clinical decision-making during operations.</p>\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6747cd6-5773-4842-b39e-782f9b0bd252",
   "metadata": {},
   "source": [
    "<body>\n",
    "\n",
    "  <h2>Dataset Overview</h2>\n",
    "<p>\n",
    "  The CholecSeg8K dataset is organized into a clear hierarchical structure, making it easy to locate and use the data. Below is a breakdown of its organization:\n",
    "</p>\n",
    "<ul>\n",
    "  <li>\n",
    "    <strong>Top-Level Directories:</strong>\n",
    "    <ul>\n",
    "      <li>Folders are labeled as <em>video01</em>, <em>video02</em>, etc., where each folder represents a complete surgical video clip.</li>\n",
    "    </ul>\n",
    "  </li>\n",
    "  <li>\n",
    "    <strong>Segment Directories:</strong>\n",
    "    <ul>\n",
    "      <li>Within each video folder, the video is divided into several segments.</li>\n",
    "      <li>Each segment directory is named with the video ID and the starting frame number (for example, <em>video01_00080</em> indicates that the segment starts at frame 80).</li>\n",
    "    </ul>\n",
    "  </li>\n",
    "  <li>\n",
    "    <strong>Frame and Image Files:</strong>\n",
    "    <ul>\n",
    "      <li>Each segment directory contains <strong>80 consecutive frames</strong> extracted from the video.</li>\n",
    "      <li>For every frame, there are <strong>4 image files</strong>:\n",
    "        <ul>\n",
    "          <li>The raw image frame</li>\n",
    "          <li>The annotation tool mask (the original hand-drawn annotation)</li>\n",
    "          <li>The color mask (used for visualization, where classes are painted in distinct colors)</li>\n",
    "          <li>The watershed mask (used for processing, where each pixel value corresponds to a class ID)</li>\n",
    "        </ul>\n",
    "      </li>\n",
    "      <li>This results in <strong>80 frames × 4 images per frame = 320 images</strong> in each segment directory.</li>\n",
    "    </ul>\n",
    "  </li>\n",
    "  <li>\n",
    "    <strong>Annotations:</strong>\n",
    "    <ul>\n",
    "      <li>Each frame is annotated at the pixel level for 13 distinct classes (e.g., tissue, instruments, blood vessels, etc.).</li>\n",
    "      <li>The annotations are presented in both the color and watershed masks, ensuring clear class identification for both visualization and automated processing.</li>\n",
    "    </ul>\n",
    "  </li>\n",
    "</ul>\n",
    "<p>\n",
    "  This structured, high-quality organization facilitates the development and training of advanced neural networks for precise semantic segmentation in surgical environments.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bfb569-7174-417f-a7a9-eb823aa4c770",
   "metadata": {},
   "source": [
    "<body>\n",
    "  <div class=\"gallery\">\n",
    "    <img src=\"./Images/Fig1.png\" alt=\"Figure 1\">\n",
    "    <img src=\"./Images/Fig2.png\" alt=\"Figure 2\">\n",
    "    <img src=\"./Images/Fig3.png\" alt=\"Figure 3\">\n",
    "  </div>\n",
    "</body>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b97e5b8-58f5-4829-a28c-83df4af1dc00",
   "metadata": {},
   "source": [
    "\n",
    "  <h2>Class Information Table</h2>\n",
    "  <p>Table I shows the corresponding class names of the class numbers in Figure 1, 2, 3 and the RGB hex code in the watershed masks:</p>\n",
    "  <table>\n",
    "    <thead>\n",
    "      <tr>\n",
    "        <th>Class Number</th>\n",
    "        <th>Class Name</th>\n",
    "        <th>RGB Hexcode</th>\n",
    "      </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "      <tr>\n",
    "        <td>Class 0</td>\n",
    "        <td>Black Background</td>\n",
    "        <td>#505050</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td>Class 1</td>\n",
    "        <td>Abdominal Wall</td>\n",
    "        <td>#111111</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td>Class 2</td>\n",
    "        <td>Liver</td>\n",
    "        <td>#212121</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td>Class 3</td>\n",
    "        <td>Gastrointestinal Tract</td>\n",
    "        <td>#131313</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td>Class 4</td>\n",
    "        <td>Fat</td>\n",
    "        <td>#121212</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td>Class 5</td>\n",
    "        <td>Grasper</td>\n",
    "        <td>#313131</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td>Class 6</td>\n",
    "        <td>Connective Tissue</td>\n",
    "        <td>#232323</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td>Class 7</td>\n",
    "        <td>Blood</td>\n",
    "        <td>#242424</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td>Class 8</td>\n",
    "        <td>Cystic Duct</td>\n",
    "        <td>#252525</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td>Class 9</td>\n",
    "        <td>L-hook Electrocautery</td>\n",
    "        <td>#323232</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td>Class 10</td>\n",
    "        <td>Gallbladder</td>\n",
    "        <td>#222222</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td>Class 11</td>\n",
    "        <td>Hepatic Vein</td>\n",
    "        <td>#333333</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td>Class 12</td>\n",
    "        <td>Liver Ligament</td>\n",
    "        <td>#050505</td>\n",
    "      </tr>\n",
    "    </tbody>\n",
    "  </table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f39d5f-5a33-4e0c-869d-e29b3e6ca0b3",
   "metadata": {},
   "source": [
    "<h2>Mask Overview</h2>\n",
    "<p>\n",
    "  The table below summarizes the three types of masks that accompany each image frame, along with their descriptions and corresponding images.\n",
    "</p>\n",
    "<table border=\"1\" cellspacing=\"0\" cellpadding=\"10\">\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>Mask Name</th>\n",
    "      <th>Description</th>\n",
    "      <th>Image</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>Original Image Frame</td>\n",
    "      <td>This is the raw endoscopic image captured during the surgery.</td>\n",
    "      <td><img src=\"./Images/frame_100_endo.png\" alt=\"Original Endoscopic Image\" width=\"200\"></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>1. Annotation Tool Mask</td>\n",
    "      <td>\n",
    "        <ul>\n",
    "          <li>This is the original hand-drawn mask created during the annotation process.</li>\n",
    "          <li>It contains detailed pixel-level annotations drawn by experts.</li>\n",
    "          <li>It serves as the basis for generating the other two masks.</li>\n",
    "        </ul>\n",
    "      </td>\n",
    "      <td><img src=\"./Images/frame_100_endo_mask.png\" alt=\"Annotation Tool Mask\" width=\"200\"></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>2. Color Mask</td>\n",
    "      <td>\n",
    "        <ul>\n",
    "          <li>Derived from the annotation tool mask.</li>\n",
    "          <li>It assigns a unique color to each class (e.g., tissue, instrument, blood vessel) based on predefined IDs.</li>\n",
    "          <li>This facilitates visual inspection and interpretation of the segmentation results.</li>\n",
    "        </ul>\n",
    "      </td>\n",
    "      <td><img src=\"./Images/frame_100_endo_color_mask.png\" alt=\"Color Mask\" width=\"200\"></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>3. Watershed Mask</td>\n",
    "      <td>\n",
    "        <ul>\n",
    "          <li>Also generated from the annotation tool mask.</li>\n",
    "          <li>It assigns a uniform pixel value (the same across all three RGB channels) to each class.</li>\n",
    "          <li>These numerical values represent the class IDs, making it ideal for automated processing and further analysis.</li>\n",
    "        </ul>\n",
    "      </td>\n",
    "      <td><img src=\"./Images/frame_100_endo_watershed_mask.png\" alt=\"Watershed Mask\" width=\"200\"></td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc5fb56e-8694-4e7e-aa2f-47cad3166e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "362f6258-1257-4604-b45e-6e672dae64e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GroupShuffleSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26241dd-0b1a-4df1-a402-f141f94d0988",
   "metadata": {},
   "source": [
    "<h2>Solution Methodology</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c274ed2-0c7c-410b-bb90-b2c8f1f502f6",
   "metadata": {},
   "source": [
    "<body>\n",
    "    <img src=\"./Images/Metodology.png\" alt=\"Fig Metodology\" width=\"300\" height=\"350\">\n",
    "</body>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23e7cb2-8892-4594-b797-244c20f414f4",
   "metadata": {},
   "source": [
    "<h2>I. Data Engineering</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf939c1-c7ed-4125-b908-4d06424d2051",
   "metadata": {},
   "source": [
    "<h3>a. Dataset Loading and Preprocessing</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f3861b-160b-4dc6-a745-d7fbdf7325ed",
   "metadata": {},
   "source": [
    "```\n",
    "Dataset/\n",
    "│\n",
    "├── video01/\n",
    "│   ├── video01_00000/\n",
    "│   │   ├── frame00000_endo.png                  ← Raw image (input)\n",
    "│   │   ├── frame00000_endo_annotation.png       ← Annotation tool mask\n",
    "│   │   ├── frame00000_endo_color_mask.png       ← Color mask (visualization)\n",
    "│   │   ├── frame00000_endo_watershed_mask.png   ← Watershed mask (labels)\n",
    "│   │   ├── frame00001_endo.png\n",
    "│   │   ├── ...\n",
    "│   │   └── frame00079_endo_watershed_mask.png\n",
    "│   ├── video01_00080/\n",
    "│   │   ├── frame00080_endo.png\n",
    "│   │   ├── ...\n",
    "│\n",
    "├── video02/\n",
    "│   ├── video02_00000/\n",
    "│   │   ├── frame00000_endo.png\n",
    "│   │   └── ...\n",
    "│\n",
    "├── ...\n",
    "│\n",
    "└── video17/\n",
    "    └── ...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d660419-5048-42f7-bca8-2847a2d75b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically detect the Dataset directory relative to the notebook's location\n",
    "dataset_dir = os.path.join(os.getcwd(), \"Dataset\")\n",
    "\n",
    "# Check if the folder exists\n",
    "if not os.path.isdir(dataset_dir):\n",
    "    raise FileNotFoundError(f\"Dataset folder not found at: {dataset_dir}\\n\"\n",
    "                            f\"Please ensure the 'Dataset' folder is placed in the same directory as this notebook.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3392cb85-6ec6-4fff-b768-fd64f9e5fcbe",
   "metadata": {},
   "source": [
    "Our data loading pipeline specifically targets two types of files:\n",
    "\n",
    "- **Original frames** — filenames containing `_endo.png` (without any suffix like `_mask` or `_annotation`).  \n",
    "  These are the raw RGB endoscopic images used as **inputs** to the model.\n",
    "\n",
    "- **Watershed masks** — filenames containing `_endo_watershed_mask.png`.  \n",
    "  These masks encode pixel-wise class IDs and are used as **ground truth labels** during training.\n",
    "\n",
    "We will gather all such image–mask pairs, ensuring that **each original image has a corresponding watershed mask** before including it in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32a6dc01-d23e-48bd-8261-7ddb1cd8ace9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect file paths for all images and their corresponding watershed masks\n",
    "image_paths = []\n",
    "mask_paths = []\n",
    "\n",
    "for root, dirs, files in os.walk(dataset_dir):\n",
    "    for filename in files:\n",
    "        \n",
    "        # Identify original image frames (filenames end with \"_endo.png\" and are not masks)\n",
    "        if filename.endswith(\"_endo.png\") and \"mask\" not in filename:\n",
    "            \n",
    "            img_path = os.path.join(root, filename)\n",
    "            \n",
    "            # Construct the corresponding watershed mask filename\n",
    "            mask_filename = filename.replace(\"_endo.png\", \"_endo_watershed_mask.png\")\n",
    "            mask_path = os.path.join(root, mask_filename)\n",
    "            \n",
    "            if os.path.exists(mask_path):\n",
    "                image_paths.append(img_path)\n",
    "                mask_paths.append(mask_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1689aef7-d11e-4809-ab82-014feca3097a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8080 image-mask pairs.\n"
     ]
    }
   ],
   "source": [
    "# Sort the paths for consistency\n",
    "image_paths.sort()\n",
    "mask_paths.sort()\n",
    "print(f\"Found {len(image_paths)} image-mask pairs.\")  # Expected: 8080 pairs for full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "235dbaa5-ad85-436a-854a-ddf8151d478b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare arrays for images and masks\n",
    "num_samples = len(image_paths)\n",
    "img_height, img_width = 256, 256\n",
    "\n",
    "X = np.zeros((num_samples, img_height, img_width, 3), dtype=np.float32)\n",
    "y = np.zeros((num_samples, img_height, img_width), dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de477861-11bd-4e08-a18a-eca53b5477f7",
   "metadata": {},
   "source": [
    "- Each pixel in a watershed mask encodes a **semantic class** using a unique **grayscale intensity**.\n",
    "- The intensity is uniform across all three channels (R = G = B), making it easy to identify programmatically.\n",
    "- These grayscale values are **mapped to integer class IDs** ranging from 0 to 12.\n",
    "- This mapping is essential for training the model using pixel-wise classification.\n",
    "- In total, there are **13 distinct classes**, including the background and various anatomical structures relevant to laparoscopic surgery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa664ac9-5d0e-4678-968f-608a91e5193a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define mapping from grayscale values to class IDs (13 classes including background)\n",
    "value_to_class = {\n",
    "    80: 0,   # background (#505050)\n",
    "    17: 1,   # abdominal wall (#111111)\n",
    "    33: 2,   # liver (#212121)\n",
    "    19: 3,   # gastrointestinal tract (#131313)\n",
    "    18: 4,   # fat (#121212)\n",
    "    49: 5,   # grasper (instrument) (#313131)\n",
    "    35: 6,   # connective tissue (#232323)\n",
    "    36: 7,   # blood (#242424)\n",
    "    37: 8,   # cystic duct (#252525)\n",
    "    50: 9,   # L-hook electrocautery (instrument) (#323232)\n",
    "    34: 10,  # gallbladder (#222222)\n",
    "    51: 11,  # hepatic vein (#333333)\n",
    "    5: 12    # liver ligament (#050505)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b1e04b-aa60-46f2-abdc-9245c48464ad",
   "metadata": {},
   "source": [
    "<p>\n",
    "Each image in the dataset is originally <strong>854×480 pixels</strong>. For training purposes, both the images and their corresponding masks are resized to <strong>256×256 pixels</strong>. This resizing is a common practice to reduce memory consumption and computational overhead, while still retaining enough detail for semantic segmentation. The chosen size of 256×256 provides a good trade-off between information preservation and model efficiency.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "Image pixel values are <strong>normalized to the [0, 1] range</strong> by dividing by 255. This standardization helps the neural network train faster and more reliably by keeping inputs on a consistent scale. \n",
    "</p>\n",
    "\n",
    "<p>\n",
    "Watershed masks are treated differently. Each pixel in the mask uses a uniform grayscale value across the R, G, and B channels (e.g., <code>[80, 80, 80]</code>). These grayscale values correspond to specific class labels. We convert each pixel to an integer class ID from <strong>0 to 12</strong> based on a predefined mapping. \n",
    "</p>\n",
    "\n",
    "<p>\n",
    "For example: <code>[80, 80, 80]</code> (hex <code>#505050</code>) represents class 0 (background), while <code>[33, 33, 33]</code> (hex <code>#212121</code>) represents class 2 (liver). This mapping is derived from the dataset documentation and ensures accurate labeling for training.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b1462aa-07a9-458e-91bd-6d8a1bc5715d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_with_aspect_ratio(img, target_size, resample):\n",
    "    original_width, original_height = img.size\n",
    "    target_width, target_height = target_size\n",
    "\n",
    "    # Calculate aspect-preserving new size\n",
    "    ratio = min(target_width / original_width, target_height / original_height)\n",
    "    new_width = int(original_width * ratio)\n",
    "    new_height = int(original_height * ratio)\n",
    "\n",
    "    # Resize while maintaining aspect ratio\n",
    "    img_resized = img.resize((new_width, new_height), resample=resample)\n",
    "\n",
    "    # Paste onto a black canvas of target size (centered)\n",
    "    new_img = Image.new(\"RGB\" if img.mode == \"RGB\" else \"L\", target_size)\n",
    "    upper_left = ((target_width - new_width) // 2, (target_height - new_height) // 2)\n",
    "    new_img.paste(img_resized, upper_left)\n",
    "\n",
    "    return new_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a3ed2b-5ac1-432c-96b8-3d57cc9b488c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (img_path, mask_path) in enumerate(zip(image_paths, mask_paths)):\n",
    "    # Load and resize the image with aspect ratio preserved\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    img = resize_with_aspect_ratio(img, (img_width, img_height), resample=Image.Resampling.BILINEAR)\n",
    "    img_array = np.array(img, dtype=np.float32) / 255.0\n",
    "    X[i] = img_array\n",
    "\n",
    "    # Load and resize the mask with nearest neighbor (preserves class labels)\n",
    "    mask = Image.open(mask_path).convert(\"L\")\n",
    "    mask = resize_with_aspect_ratio(mask, (img_width, img_height), resample=Image.Resampling.NEAREST)\n",
    "    mask_array = np.array(mask, dtype=np.uint8)\n",
    "\n",
    "    # Map grayscale values to class IDs\n",
    "    mask_mapped = np.zeros_like(mask_array, dtype=np.uint8)\n",
    "    for val, cls in value_to_class.items():\n",
    "        mask_mapped[mask_array == val] = cls\n",
    "    y[i] = mask_mapped\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28eec34f-f0de-48d7-a66c-db36ac73064b",
   "metadata": {},
   "source": [
    "During preprocessing, each image–mask pair is resized to a fixed dimension of **256×256 pixels** using different interpolation strategies for the input image and its corresponding label mask:\n",
    "\n",
    "- **Image resizing:**\n",
    "  - Each RGB image is loaded and converted to a 3-channel format (`RGB`).\n",
    "  - It is then resized using **bilinear interpolation** (`Image.Resampling.BILINEAR`), which computes the output pixel value as a weighted average of the nearest four pixels. \n",
    "  - This method preserves smooth gradients and is suitable for natural images, making it ideal for input data to convolutional networks.\n",
    "  - After resizing, the image is normalized to the **[0, 1]** range by dividing by 255, ensuring consistent input scale for the neural network.\n",
    "\n",
    "- **Mask resizing:**\n",
    "  - Each watershed mask is loaded in **grayscale** mode (`\"L\"`), resulting in a single-channel image.\n",
    "  - It is resized using **nearest-neighbor interpolation** (`Image.Resampling.NEAREST`) to preserve **discrete class boundaries**. This avoids introducing interpolated pixel values that could distort class labels.\n",
    "  - The resized mask is then converted from grayscale pixel values to **class IDs** (0 to 12) using a predefined mapping, ensuring the model receives accurate training labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2293a04-d3bd-4a36-9174-fbf154f61e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "print(\"Data loaded successfully!\")\n",
    "print(\"Shape of X (images):\", X.shape)\n",
    "print(\"Shape of y (masks):\", y.shape)\n",
    "print(\"Image pixel range [min, max]:\", X.min(), X.max())\n",
    "print(\"Unique mask labels:\", np.unique(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546e8c17-e423-4d4f-ab2b-48f918eb0f6b",
   "metadata": {},
   "source": [
    "<h3>b. Data splitting</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a4ddf9-1412-4e04-8346-b5f1595690ad",
   "metadata": {},
   "source": [
    "To evaluate our model's performance effectively, we divide the dataset into three parts:\n",
    "\n",
    "- **Training set (60%)**  \n",
    "  Used to train the neural network and update weights during learning.\n",
    "\n",
    "- **Validation set (20%)**  \n",
    "  Used during training to monitor model performance, tune hyperparameters, and apply early stopping.\n",
    "\n",
    "- **Test set (20%)**  \n",
    "  Set aside until the very end. Used to evaluate the model's true generalization performance on completely unseen data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577ba43a-30c0-4d20-a6c1-6b8a8c42ce0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build group labels for each sample based on the video folder\n",
    "groups = [os.path.relpath(p, dataset_dir).split(os.sep)[0] for p in image_paths]  # e.g., \"video01\", \"video02\", etc.\n",
    "\n",
    "# First split: train_val (80%) vs test (20%), grouped by video\n",
    "gss1 = GroupShuffleSplit(test_size=0.20, n_splits=1, random_state=42)\n",
    "trainval_indices, test_indices = next(gss1.split(X, y, groups=groups))\n",
    "X_train_val, X_test = X[trainval_indices], X[test_indices]\n",
    "y_train_val, y_test = y[trainval_indices], y[test_indices]\n",
    "\n",
    "# Second split: from train_val, get train (75% of train_val -> 60% overall) vs val (25% of train_val -> 20% overall), also grouped by video\n",
    "trainval_groups = np.array(groups)[trainval_indices]  # group labels for the train_val subset\n",
    "gss2 = GroupShuffleSplit(test_size=0.25, n_splits=1, random_state=42)\n",
    "train_indices, val_indices = next(gss2.split(X_train_val, y_train_val, groups=trainval_groups))\n",
    "X_train, X_val = X_train_val[train_indices], X_train_val[val_indices]\n",
    "y_train, y_val = y_train_val[train_indices], y_train_val[val_indices]\n",
    "\n",
    "# Confirm the new split sizes (all frames of a video are confined to one split)\n",
    "print(f\"Training set: {X_train.shape[0]} images\")\n",
    "print(f\"Validation set: {X_val.shape[0]} images\")\n",
    "print(f\"Test set: {X_test.shape[0]} images\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cefe86-9050-4850-99d3-7496bdd1132e",
   "metadata": {},
   "source": [
    "<h2>II. Model Architecture: U-Net for Semantic Segmentation</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536e8425-c0df-43a4-8b53-9dcb1c11ff03",
   "metadata": {},
   "source": [
    "<h3>a. Architecture Overview</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f9075d-f2b6-4251-9616-604322ae7a05",
   "metadata": {},
   "source": [
    "For this project, we use a **U-Net architecture** — a popular encoder–decoder convolutional neural network designed for semantic segmentation tasks in biomedical imaging.\n",
    "\n",
    "U-Net is built to capture both global context and fine-grained local details, thanks to its **skip connections** that link the encoder and decoder paths.\n",
    "\n",
    "- **Encoder (Contracting Path):**\n",
    "  - Consists of **4 downsampling blocks**\n",
    "  - Each block includes:\n",
    "    - Two `3×3` convolutional layers with ReLU activation\n",
    "    - A `2×2` max pooling layer for spatial downsampling\n",
    "  - The number of filters doubles at each stage:  \n",
    "    `64 → 128 → 256 → 512`\n",
    "  - After the fourth block, there's a **bottleneck layer** with `1024` filters\n",
    "\n",
    "- **Decoder (Expanding Path):**\n",
    "  - Consists of **4 upsampling blocks**\n",
    "  - Each block includes:\n",
    "    - A `2×2` transposed convolution to upsample and halve the number of filters\n",
    "    - A skip connection that concatenates the corresponding feature map from the encoder\n",
    "    - Two `3×3` convolutional layers with ReLU activation\n",
    "  - Filter sizes follow the reverse order:  \n",
    "    `1024 → 512 → 256 → 128 → 64`\n",
    "\n",
    "- **Output Layer:**\n",
    "  - A `1×1` convolution to reduce the channel dimension to the number of classes (**13**)\n",
    "  - Followed by a **softmax activation** to produce a probability distribution per pixel\n",
    "\n",
    "This architecture enables the model to segment objects at different scales and accurately preserve spatial information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2b581a-69ad-48dd-848f-6394c82beb2a",
   "metadata": {},
   "source": [
    "<body>\n",
    "    <img src=\"./Images/U-Net-convolutional-neural-network-architecture.png\" alt=\"Fig Architecture\">\n",
    "</body>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df39afd6-7ce6-41df-a92e-41f933aae820",
   "metadata": {},
   "source": [
    "*The diagram above closely resembles our architecture, with two main differences: our model uses only two consecutive convolutional layers per block (instead of three), and the number of filters in each convolutional layer is slightly different.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebc4966-9e36-4b0d-b945-1582ea4b120d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, Input, Model\n",
    "\n",
    "def build_unet(input_size=(256, 256, 3), num_classes=13):\n",
    "    inputs = Input(shape=input_size)\n",
    "    # Encoder (downsampling path)\n",
    "    c1 = layers.Conv2D(64, 3, activation='relu', padding='same')(inputs)\n",
    "    c1 = layers.Conv2D(64, 3, activation='relu', padding='same')(c1)\n",
    "    p1 = layers.MaxPooling2D((2, 2))(c1)\n",
    "\n",
    "    c2 = layers.Conv2D(128, 3, activation='relu', padding='same')(p1)\n",
    "    c2 = layers.Conv2D(128, 3, activation='relu', padding='same')(c2)\n",
    "    p2 = layers.MaxPooling2D((2, 2))(c2)\n",
    "\n",
    "    c3 = layers.Conv2D(256, 3, activation='relu', padding='same')(p2)\n",
    "    c3 = layers.Conv2D(256, 3, activation='relu', padding='same')(c3)\n",
    "    p3 = layers.MaxPooling2D((2, 2))(c3)\n",
    "\n",
    "    c4 = layers.Conv2D(512, 3, activation='relu', padding='same')(p3)\n",
    "    c4 = layers.Conv2D(512, 3, activation='relu', padding='same')(c4)\n",
    "    p4 = layers.MaxPooling2D((2, 2))(c4)\n",
    "\n",
    "    # Bottleneck\n",
    "    c5 = layers.Conv2D(1024, 3, activation='relu', padding='same')(p4)\n",
    "    c5 = layers.Conv2D(1024, 3, activation='relu', padding='same')(c5)\n",
    "    c5 = layers.Dropout(0.5)(c5)  # **Dropout added to bottleneck layer**\n",
    "\n",
    "    # Decoder (upsampling path + skip connections)\n",
    "    u6 = layers.Conv2DTranspose(512, 2, strides=2, padding='same')(c5)\n",
    "    u6 = layers.concatenate([u6, c4])        # skip connection from encoder c4\n",
    "    c6 = layers.Conv2D(512, 3, activation='relu', padding='same')(u6)\n",
    "    c6 = layers.Conv2D(512, 3, activation='relu', padding='same')(c6)\n",
    "\n",
    "    u7 = layers.Conv2DTranspose(256, 2, strides=2, padding='same')(c6)\n",
    "    u7 = layers.concatenate([u7, c3])\n",
    "    c7 = layers.Conv2D(256, 3, activation='relu', padding='same')(u7)\n",
    "    c7 = layers.Conv2D(256, 3, activation='relu', padding='same')(c7)\n",
    "\n",
    "    u8 = layers.Conv2DTranspose(128, 2, strides=2, padding='same')(c7)\n",
    "    u8 = layers.concatenate([u8, c2])\n",
    "    c8 = layers.Conv2D(128, 3, activation='relu', padding='same')(u8)\n",
    "    c8 = layers.Conv2D(128, 3, activation='relu', padding='same')(c8)\n",
    "\n",
    "    u9 = layers.Conv2DTranspose(64, 2, strides=2, padding='same')(c8)\n",
    "    u9 = layers.concatenate([u9, c1])\n",
    "    c9 = layers.Conv2D(64, 3, activation='relu', padding='same')(u9)\n",
    "    c9 = layers.Conv2D(64, 3, activation='relu', padding='same')(c9)\n",
    "\n",
    "    # Output layer\n",
    "    outputs = layers.Conv2D(num_classes, 1, activation='softmax')(c9)\n",
    "    return Model(inputs, outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1dfb6b-c3e5-4e6c-bf9d-3898de4f15a2",
   "metadata": {},
   "source": [
    "<h3>b. Model Compilation and Training Setup</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a3fc05-e843-478f-a882-b392c68d8d31",
   "metadata": {},
   "source": [
    "- **Loss Function:**  \n",
    "  `SparseCategoricalCrossentropy` is used since the target masks contain integer class labels (not one-hot encoded).\n",
    "\n",
    "- **Optimizer:**  \n",
    "  `Adam` — a robust and widely used optimizer for deep learning, with a default learning rate.\n",
    "\n",
    "- **Metrics:**  \n",
    "  We track **pixel-wise accuracy** during training.  \n",
    "  More detailed metrics like **IoU** and **Dice coefficient** will be computed separately after training.\n",
    "\n",
    "- **Early Stopping:**  \n",
    "  To prevent overfitting, we use early stopping with `patience = 5`.  \n",
    "  This means training will stop if the validation loss does not improve for 5 consecutive epochs. The best-performing model weights are automatically restored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917b47be-2323-4972-8348-e3856b08cf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the U-Net model and compile it\n",
    "num_classes = 13\n",
    "model = build_unet(input_size=(256, 256, 3), num_classes=num_classes)\n",
    "model.compile(optimizer=keras.optimizers.Adam(),\n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf0900e-e592-4580-a204-a5b0c41a5054",
   "metadata": {},
   "source": [
    "<h3>c. Model Training</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c18e4e9-0d74-4c62-a98c-68dd03fe5f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up early stopping callback\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=20,\n",
    "                    epochs=2,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    callbacks=[early_stop])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25df7027-6007-4c0c-beec-a920628ae67c",
   "metadata": {},
   "source": [
    "<h3>d. Final Evaluation on the Test Set</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15048538-28f6-4444-b003-b3aa297020d6",
   "metadata": {},
   "source": [
    "Evaluate the model's true generalization performance on the remaining 20% test set, which has remained untouched until now — it was not used for training or weight updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d721d776-2d3a-427a-9218-f3219a86ff75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on the test set (20%)\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0afc58-c2fc-4371-a665-93e20bfc824e",
   "metadata": {},
   "source": [
    "<h2>III. Evaluation Metrics</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c89aa51-9491-4e0a-8f6c-85e31bd7de13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "y_pred_probs = model.predict(X_test, batch_size=4)\n",
    "y_pred = np.argmax(y_pred_probs, axis=-1)  # shape: (num_test, 256, 256)\n",
    "\n",
    "# Ground truth from the test set\n",
    "y_true = y_test  # already in (num_test, 256, 256) format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9556a36c-05c1-4e7f-a6b1-2340b295f332",
   "metadata": {},
   "source": [
    "<h3>a. Pixel Accuracy</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222040b5-6af7-46fa-9881-4f935775c158",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "**Pixel Accuracy**: the percentage of pixels (over the whole image set) whose predicted class matches the ground truth class. This is a global measure and was also tracked during training as the 'accuracy' metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cb08a0-af4e-4592-9e04-756042801b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute pixel accuracy\n",
    "total_pixels = y_true.size\n",
    "correct_pixels = np.sum(y_pred == y_true)\n",
    "pixel_accuracy = correct_pixels / total_pixels\n",
    "\n",
    "print(f\"Test Pixel Accuracy: {pixel_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e804a08-2e4d-4bda-9475-250f90132f2f",
   "metadata": {},
   "source": [
    "<h3>b. Intersection over Union (IoU)</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9eb5e2f-f87f-442b-b9c4-d858c6840093",
   "metadata": {},
   "source": [
    "**Intersection over Union (IoU)**: for each class, IoU = (True Positive) / (True Positive + False Positive + False Negative), i.e., the area of overlap between the predicted mask and true mask divided by the area of their union. We will compute the IoU for each class and then take the average (Mean IoU) across all classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bbf12f-49ff-40a6-9b58-21e4883241f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of classes (from the CholecSeg8K dataset)\n",
    "num_classes = 13\n",
    "iou_per_class = []\n",
    "\n",
    "# Compute IoU for each class on the test set\n",
    "for cls in range(num_classes):\n",
    "    pred_mask = (y_pred == cls)\n",
    "    true_mask = (y_true == cls)\n",
    "    \n",
    "    intersection = np.logical_and(pred_mask, true_mask).sum()\n",
    "    union = np.logical_or(pred_mask, true_mask).sum()\n",
    "    \n",
    "    if union == 0:\n",
    "        # Skip this class if it doesn't appear in either prediction or ground truth\n",
    "        continue\n",
    "\n",
    "    iou = intersection / union\n",
    "    iou_per_class.append(iou)\n",
    "\n",
    "# Mean IoU across all classes that were present\n",
    "mean_iou = np.mean(iou_per_class)\n",
    "\n",
    "print(f\"Test Mean IoU: {mean_iou:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f7c4e2-9b95-4a08-b124-a4a1e09d134f",
   "metadata": {},
   "source": [
    "<h3>c. Dice Coefficient</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9166ce8-72b2-4311-acd2-e110bb5bd21b",
   "metadata": {},
   "source": [
    "**Dice Coefficient**: also known as F1 score for segmentation, Dice = 2 * (Precision * Recall) / (Precision + Recall) for each class, which can be computed as 2 * |Prediction ∩ Ground Truth| / (|Prediction| + |Ground Truth|). We will compute Dice per class and then average. Dice is closely related to IoU (Dice = 2*IoU/(IoU+1)) and emphasizes overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8263b93-27a1-4db3-a43d-46b7f8141113",
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_per_class = []\n",
    "\n",
    "# Compute Dice coefficient for each class\n",
    "for cls in range(num_classes):\n",
    "    pred_mask = (y_pred == cls)\n",
    "    true_mask = (y_true == cls)\n",
    "    \n",
    "    intersection = np.logical_and(pred_mask, true_mask).sum()\n",
    "    pred_area = pred_mask.sum()\n",
    "    true_area = true_mask.sum()\n",
    "    \n",
    "    # Skip class if not present in both prediction and ground truth\n",
    "    if true_area == 0 and pred_area == 0:\n",
    "        continue\n",
    "    \n",
    "    # Dice formula: 2 * |A ∩ B| / (|A| + |B|)\n",
    "    dice = (2 * intersection) / (pred_area + true_area + 1e-8)\n",
    "    dice_per_class.append(dice)\n",
    "\n",
    "# Mean Dice across all valid classes\n",
    "mean_dice = np.mean(dice_per_class)\n",
    "\n",
    "# Final evaluation summary\n",
    "print(f\"Test Pixel Accuracy: {pixel_accuracy * 100:.2f}%\")\n",
    "print(f\"Test Mean IoU: {mean_iou * 100:.2f}%\")\n",
    "print(f\"Test Mean Dice Coefficient: {mean_dice * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58e0a24-7020-46f8-b5ce-afadc7f2bc79",
   "metadata": {},
   "source": [
    "<h2>IV. Results Visualization</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1119dc67-0a7f-4b48-90d8-b8796a64bef1",
   "metadata": {},
   "source": [
    "To get a better intuition of the model's performance, let's visualize some segmentation results. We'll take a few examples from the validation set and display:\n",
    "- The original image.\n",
    "- The ground truth mask (using a color overlay for different classes).\n",
    "- The predicted mask from our model (using the same color scheme as ground truth for easy comparison)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa939a4-ee4b-4cb0-b089-d19e15bf041d",
   "metadata": {},
   "source": [
    "We'll use a consistent color palette to color-code the 13 classes. For clarity, let's define a set of distinct colors for the classes. (These are arbitrary chosen colors for visualization; they may not match the exact colors used in the dataset's color mask, but serve to differentiate classes.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b9d1c8-b685-4e9d-8170-60b4d6556a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the color map for classes (as before)\n",
    "class_colors = [\n",
    "    (0, 0, 0),        # 0: background (black)\n",
    "    (128, 64, 128),   # 1: abdominal wall (purple)\n",
    "    (128, 128, 64),   # 2: liver (olive)\n",
    "    (60, 180, 75),    # 3: gastrointestinal tract (green)\n",
    "    (255, 225, 25),   # 4: fat (yellow)\n",
    "    (0, 130, 200),    # 5: grasper (blue)\n",
    "    (245, 130, 48),   # 6: connective tissue (orange)\n",
    "    (220, 20, 60),    # 7: blood (red/crimson)\n",
    "    (230, 190, 255),  # 8: cystic duct (lavender)\n",
    "    (170, 110, 40),   # 9: electrocautery (brown)\n",
    "    (0, 0, 255),      # 10: gallbladder (bright blue)\n",
    "    (128, 0, 0),      # 11: hepatic vein (maroon)\n",
    "    (170, 255, 195)   # 12: liver ligament (mint green)\n",
    "]\n",
    "\n",
    "# Choose some sample indices from the test set to visualize\n",
    "sample_indices = [0, 1, 2]  # (for example, first 3 test images)\n",
    "\n",
    "# Set up a figure with one row per sample and 2 columns (original, predicted mask)\n",
    "n_samples = len(sample_indices)\n",
    "fig, axes = plt.subplots(n_samples, 2, figsize=(8, 4 * n_samples))\n",
    "\n",
    "for i, idx in enumerate(sample_indices):\n",
    "    image = X_test[idx]\n",
    "    true_mask = y_test[idx]        # ground truth mask (if needed for reference)\n",
    "    pred_mask = y_pred[idx]        # model’s predicted mask (class IDs per pixel)\n",
    "\n",
    "    # Map class IDs to colors for the predicted mask\n",
    "    H, W = pred_mask.shape\n",
    "    pred_mask_color = np.zeros((H, W, 3), dtype=np.uint8)\n",
    "    for cls, color in enumerate(class_colors):\n",
    "        pred_mask_color[pred_mask == cls] = color\n",
    "\n",
    "    # Plot original image\n",
    "    axes[i, 0].imshow(image)\n",
    "    axes[i, 0].set_title(\"Original Image\")\n",
    "    axes[i, 0].axis('off')\n",
    "\n",
    "    # Plot predicted segmentation mask (colorized)\n",
    "    axes[i, 1].imshow(pred_mask_color)\n",
    "    axes[i, 1].set_title(\"Predicted Mask\")\n",
    "    axes[i, 1].axis('off')\n",
    "\n",
    "# Tight layout for better spacing\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c8471f-ba02-43fe-9ad1-c19025b6adcc",
   "metadata": {},
   "source": [
    "<h2>V. Deploy</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9578ceb3-964a-4bc8-adf9-817ed899e622",
   "metadata": {},
   "source": [
    "<p>Finally, we save the trained model to disk so that it can be reloaded later for inference or further training without having to retrain from scratch. We'll save the model in Keras's HDF5 format:<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b3bfc1-318c-45f5-8c16-d5df4a11c6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"cholecseg_unet_model.h5\")\n",
    "print(\"Model saved to disk.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
